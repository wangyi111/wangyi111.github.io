{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2EGZp4-yuLy",
        "colab_type": "text"
      },
      "source": [
        "This is a brief summary for the implementation of tensorflow, from data preparation to the usage of trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG998fB517hz",
        "colab_type": "text"
      },
      "source": [
        "## 01: Prepare Data\n",
        "\n",
        "In tensorflow the API tf.data constructs input data pipeline to help manage huge volume of data with various formats and conversions.\n",
        "\n",
        "Data pipeline could be constructed through following methods: numpy array, pandas DataFrame, Python generator, csv file, text file, file path, tfrecords file.\n",
        "\n",
        "Among these methods, the most popular ones are: numpy array, pandas DataFrame and file path. For very large data volume, the tfrecord is useful. The advantage of using tfrecords files is its small volume after compression, its convenient sharing through the Internet, and the fast speed of loading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbAD7kcA3XHt",
        "colab_type": "text"
      },
      "source": [
        "### 1-1: Numpy Array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzzeXSnUyl42",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "d32ba08b-e4e8-4b81-fb3b-689db68002a9"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np \n",
        "from sklearn import datasets \n",
        "iris = datasets.load_iris()\n",
        "\n",
        "ds1 = tf.data.Dataset.from_tensor_slices((iris[\"data\"],iris[\"target\"])) # dataset from numpy array\n",
        "for features,label in ds1.take(5):\n",
        "    print(features,label)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([5.1 3.5 1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor([4.9 3.  1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor([4.7 3.2 1.3 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor([4.6 3.1 1.5 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n",
            "tf.Tensor([5.  3.6 1.4 0.2], shape=(4,), dtype=float64) tf.Tensor(0, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6J8T3Ny3tpt",
        "colab_type": "text"
      },
      "source": [
        "### 1-2: Pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiU5rcPf31Al",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "ccfe1002-05aa-43fd-c854-c44ea0d13fa3"
      },
      "source": [
        "from sklearn import datasets \n",
        "import pandas as pd\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "dfiris = pd.DataFrame(iris[\"data\"],columns = iris.feature_names)\n",
        "ds2 = tf.data.Dataset.from_tensor_slices((dfiris.to_dict(\"list\"),iris[\"target\"]))\n",
        "\n",
        "for features,label in ds2.take(3):\n",
        "    print(features,label)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=5.1>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.5>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.4>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)\n",
            "{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=4.9>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.0>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.4>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)\n",
            "{'sepal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=4.7>, 'sepal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=3.2>, 'petal length (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=1.3>, 'petal width (cm)': <tf.Tensor: shape=(), dtype=float32, numpy=0.2>} tf.Tensor(0, shape=(), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOa9IOIJ5IJE",
        "colab_type": "text"
      },
      "source": [
        "### 1-3: Python generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mvk1XzI4Hu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Defining a generator to read image from a folder\n",
        "image_generator = ImageDataGenerator(rescale=1.0/255).flow_from_directory(\n",
        "                    \"../data/cifar2/test/\",\n",
        "                    target_size=(32, 32),\n",
        "                    batch_size=20,\n",
        "                    class_mode='binary')\n",
        "\n",
        "classdict = image_generator.class_indices\n",
        "print(classdict)\n",
        "\n",
        "def generator():\n",
        "    for features,label in image_generator:\n",
        "        yield (features,label)\n",
        "\n",
        "ds3 = tf.data.Dataset.from_generator(generator,output_types=(tf.float32,tf.int32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_rYuARA5q66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "plt.figure(figsize=(6,6)) \n",
        "for i,(img,label) in enumerate(ds3.unbatch().take(9)):\n",
        "    ax=plt.subplot(3,3,i+1)\n",
        "    ax.imshow(img.numpy())\n",
        "    ax.set_title(\"label = %d\"%label)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([]) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej7-bOcK7SGJ",
        "colab_type": "text"
      },
      "source": [
        "### 1-4: csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYgCumciaWOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds4 = tf.data.experimental.make_csv_dataset(\n",
        "      file_pattern = [\"../data/titanic/train.csv\",\"../data/titanic/test.csv\"],\n",
        "      batch_size=3, \n",
        "      label_name=\"Survived\",\n",
        "      na_value=\"\",\n",
        "      num_epochs=1,\n",
        "      ignore_errors=True)\n",
        "\n",
        "for data,label in ds4.take(2):\n",
        "    print(data,label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4WqUA0F7yv3",
        "colab_type": "text"
      },
      "source": [
        "### 1-5: txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjEc3ADIalj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds5 = tf.data.TextLineDataset(\n",
        "    filenames = [\"../data/titanic/train.csv\",\"../data/titanic/test.csv\"]\n",
        "    ).skip(1) # Omitting the header on the first line\n",
        "\n",
        "for line in ds5.take(5):\n",
        "    print(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0Et7zxf8KGK",
        "colab_type": "text"
      },
      "source": [
        "### 1-6: file path"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6cDdxBb74eV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds6 = tf.data.Dataset.list_files(\"../data/cifar2/train/*/*.jpg\")\n",
        "for file in ds6.take(5):\n",
        "    print(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOxy6mTN8WtI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt \n",
        "def load_image(img_path,size = (32,32)):\n",
        "    label = 1 if tf.strings.regex_full_match(img_path,\".*/automobile/.*\") else 0\n",
        "    img = tf.io.read_file(img_path)\n",
        "    img = tf.image.decode_jpeg(img) # Note that we are using jpeg format\n",
        "    img = tf.image.resize(img,size)\n",
        "    return(img,label)\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "for i,(img,label) in enumerate(ds6.map(load_image).take(2)):\n",
        "    plt.figure(i)\n",
        "    plt.imshow((img/255.0).numpy())\n",
        "    plt.title(\"label = %d\"%label)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6HpNZbz-Bic",
        "colab_type": "text"
      },
      "source": [
        "### 1-7: tfrecords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PJRFw-j-LoH",
        "colab_type": "text"
      },
      "source": [
        "For the data pipeline with tfrecords file it is a little bit complicated, as we have to first create tfrecords file, and then read it to tf.data.dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU6bB9m98ZDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# inpath is the original data path; outpath: output path of the TFRecord file\n",
        "def create_tfrecords(inpath,outpath): \n",
        "    writer = tf.io.TFRecordWriter(outpath)\n",
        "    dirs = os.listdir(inpath)\n",
        "    for index, name in enumerate(dirs):\n",
        "        class_path = inpath +\"/\"+ name+\"/\"\n",
        "        for img_name in os.listdir(class_path):\n",
        "            img_path = class_path + img_name\n",
        "            img = tf.io.read_file(img_path)\n",
        "            #img = tf.image.decode_image(img)\n",
        "            #img = tf.image.encode_jpeg(img) # Use jpeg format for all the compressions\n",
        "            example = tf.train.Example(\n",
        "               features=tf.train.Features(feature={\n",
        "                    'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[index])),\n",
        "                    'img_raw': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img.numpy()]))\n",
        "               }))\n",
        "            writer.write(example.SerializeToString())\n",
        "    writer.close()\n",
        "    \n",
        "create_tfrecords(\"../data/cifar2/test/\",\"../data/cifar2_test.tfrecords/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g6TYEBS-pq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt \n",
        "\n",
        "def parse_example(proto):\n",
        "    description ={ 'img_raw' : tf.io.FixedLenFeature([], tf.string),\n",
        "                   'label': tf.io.FixedLenFeature([], tf.int64)} \n",
        "    example = tf.io.parse_single_example(proto, description)\n",
        "    img = tf.image.decode_jpeg(example[\"img_raw\"])   # Note that we are using jpeg format\n",
        "    img = tf.image.resize(img, (32,32))\n",
        "    label = example[\"label\"]\n",
        "    return(img,label)\n",
        "\n",
        "ds7 = tf.data.TFRecordDataset(\"../data/cifar2_test.tfrecords\").map(parse_example).shuffle(3000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmlQ83yY-xML",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'svg'\n",
        "plt.figure(figsize=(6,6)) \n",
        "for i,(img,label) in enumerate(ds7.take(9)):\n",
        "    ax=plt.subplot(3,3,i+1)\n",
        "    ax.imshow((img/255.0).numpy())\n",
        "    ax.set_title(\"label = %d\"%label)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([]) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kG1b29cX-9vh",
        "colab_type": "text"
      },
      "source": [
        "Meanwhile, tf.data.dataset also has many functions for data conversion/preprocessing. Examples are listed as follows:\n",
        "\n",
        "* `map`: projecting the conversion function to every element in the dataset.\n",
        "\n",
        "* `flat_map`: projecting the conversion function to every element in the dataset, and flatten the embedded Dataset.\n",
        "\n",
        "* `interleave`: similar as `flat_map` but interleaves the data from different sources.\n",
        "\n",
        "* `filter`: filter certain elements.\n",
        "\n",
        "* `zip`: zipping two Datasets with the same length.\n",
        "\n",
        "* `concatenate`: concatenating two Datasets.\n",
        "\n",
        "* `reduce`: executing operation of reducing.\n",
        "\n",
        "* `batch`: constructing batches and release one batch each time; there will be one more rank comparing to the original data; the inverse operation is `unbatch`.\n",
        "\n",
        "* `padded_batch`: constructing batches, similar as `batch`, but can achieve padded shape.\n",
        "\n",
        "* `window`: constructing sliding window, and return Dataset of Dataset.\n",
        "\n",
        "* `shuffle`: shuffling the order of the data.\n",
        "\n",
        "* `repeat`: repeat the data certain times; if no argument is specified, repeat data with infinitive times.\n",
        "\n",
        "* `shard`: sampling the elements starting from a certain position with fixed distance.\n",
        "\n",
        "* `take`: sampling the first few elements from a certain position."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PKsV4KM_5ms",
        "colab_type": "text"
      },
      "source": [
        "## 02: Build model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbzQtYDwARhY",
        "colab_type": "text"
      },
      "source": [
        "There are three ways of modeling: using `Sequential` to construct model with the order of layers, using functional APIs to construct model with arbitrary structure, using child class inheriting from the base class `Model`.\n",
        "\n",
        "For the models with sequenced structure, `Sequential` method should be given the highest priority.\n",
        "\n",
        "For the models with nonsequenced structures such as multiple input/output, shared weights, or residual connections, modeling with functional API is recommended.\n",
        "\n",
        "Modeling through child class of `Model` should be AVOIDED unless with special requirements. This method is flexible, but also fallible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpbf5DS6AgPG",
        "colab_type": "text"
      },
      "source": [
        "### 2-1: Sequential"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5oUL1zn_kjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Embedding(MAX_WORDS,7,input_length=MAX_LEN))\n",
        "model.add(layers.Conv1D(filters = 64,kernel_size = 5,activation = \"relu\"))\n",
        "model.add(layers.MaxPool1D(2))\n",
        "model.add(layers.Conv1D(filters = 32,kernel_size = 3,activation = \"relu\"))\n",
        "model.add(layers.MaxPool1D(2))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(1,activation = \"sigmoid\"))\n",
        "\n",
        "model.compile(optimizer='Nadam',\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy',\"AUC\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuZUBx1CAv5M",
        "colab_type": "text"
      },
      "source": [
        "### 2-2: functional API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQg6J7wMAms1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "inputs = layers.Input(shape=[MAX_LEN])\n",
        "x  = layers.Embedding(MAX_WORDS,7)(inputs)\n",
        "\n",
        "branch1 = layers.SeparableConv1D(64,3,activation=\"relu\")(x)\n",
        "branch1 = layers.MaxPool1D(3)(branch1)\n",
        "branch1 = layers.SeparableConv1D(32,3,activation=\"relu\")(branch1)\n",
        "branch1 = layers.GlobalMaxPool1D()(branch1)\n",
        "\n",
        "branch2 = layers.SeparableConv1D(64,5,activation=\"relu\")(x)\n",
        "branch2 = layers.MaxPool1D(5)(branch2)\n",
        "branch2 = layers.SeparableConv1D(32,5,activation=\"relu\")(branch2)\n",
        "branch2 = layers.GlobalMaxPool1D()(branch2)\n",
        "\n",
        "branch3 = layers.SeparableConv1D(64,7,activation=\"relu\")(x)\n",
        "branch3 = layers.MaxPool1D(7)(branch3)\n",
        "branch3 = layers.SeparableConv1D(32,7,activation=\"relu\")(branch3)\n",
        "branch3 = layers.GlobalMaxPool1D()(branch3)\n",
        "\n",
        "concat = layers.Concatenate()([branch1,branch2,branch3])\n",
        "outputs = layers.Dense(1,activation = \"sigmoid\")(concat)\n",
        "\n",
        "model = models.Model(inputs = inputs,outputs = outputs)\n",
        "\n",
        "model.compile(optimizer='Nadam',\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy',\"AUC\"])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot5Kd6Q7DkdG",
        "colab_type": "text"
      },
      "source": [
        "### 2-3: Customized Modeling Using Child Class of Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQfqtsr3A-YS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a customized residual module as Layer\n",
        "\n",
        "class ResBlock(layers.Layer):\n",
        "    def __init__(self, kernel_size, **kwargs):\n",
        "        super(ResBlock, self).__init__(**kwargs)\n",
        "        self.kernel_size = kernel_size\n",
        "    \n",
        "    def build(self,input_shape):\n",
        "        self.conv1 = layers.Conv1D(filters=64,kernel_size=self.kernel_size,\n",
        "                                   activation = \"relu\",padding=\"same\")\n",
        "        self.conv2 = layers.Conv1D(filters=32,kernel_size=self.kernel_size,\n",
        "                                   activation = \"relu\",padding=\"same\")\n",
        "        self.conv3 = layers.Conv1D(filters=input_shape[-1],\n",
        "                                   kernel_size=self.kernel_size,activation = \"relu\",padding=\"same\")\n",
        "        self.maxpool = layers.MaxPool1D(2)\n",
        "        super(ResBlock,self).build(input_shape) # Identical to self.built = True\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = layers.Add()([inputs,x])\n",
        "        x = self.maxpool(x)\n",
        "        return x\n",
        "    \n",
        "    # Need to define get_config method in order to sequentialize the model constructed from the customized Layer by Functional API.\n",
        "    def get_config(self):  \n",
        "        config = super(ResBlock, self).get_config()\n",
        "        config.update({'kernel_size': self.kernel_size})\n",
        "        return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXrddOTfD_C7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Customized model, which could also be implemented by Sequential or Functional API\n",
        "\n",
        "class ImdbModel(models.Model):\n",
        "    def __init__(self):\n",
        "        super(ImdbModel, self).__init__()\n",
        "        \n",
        "    def build(self,input_shape):\n",
        "        self.embedding = layers.Embedding(MAX_WORDS,7)\n",
        "        self.block1 = ResBlock(7)\n",
        "        self.block2 = ResBlock(5)\n",
        "        self.dense = layers.Dense(1,activation = \"sigmoid\")\n",
        "        super(ImdbModel,self).build(input_shape)\n",
        "    \n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = layers.Flatten()(x)\n",
        "        x = self.dense(x)\n",
        "        return(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__QVoSWfEd6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = ImdbModel()\n",
        "model.build(input_shape =(None,200))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='Nadam',\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy',\"AUC\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaY5iYW5GDTd",
        "colab_type": "text"
      },
      "source": [
        "## 03: Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoIHi80dGY_b",
        "colab_type": "text"
      },
      "source": [
        "There are three ways of model training: using pre-defined fit method, using pre-defined tran_on_batch method, using customized training loop.\n",
        "\n",
        "Note: fit_generator method is not recommended in tf.keras since it has been merged into fit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grsuX27aGqgr",
        "colab_type": "text"
      },
      "source": [
        "### 3-1: model.fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx8ndRKSGxbL",
        "colab_type": "text"
      },
      "source": [
        "build model -> compile model -> train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ0anRLCGdi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_model() # create model using methods listed above\n",
        "model.compile(optimizer=optimizers.Nadam(),loss=losses.SparseCategoricalCrossentropy(),metrics=['accuracy']) # compile model\n",
        "\n",
        "logdir = \"./data/keras_model/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1) # tensorboard\n",
        "\n",
        "history = model.fit(ds_train,validation_data = ds_test,epochs = 10,callbacks=[tensorboard_callback]) # train model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enCewLWeIfKr",
        "colab_type": "text"
      },
      "source": [
        "### 3-2: train_on_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mS7Cc_sIu6F",
        "colab_type": "text"
      },
      "source": [
        "This pre-defined method allows fine-controlling to the training procedure for each batch without the callbacks, which is even more flexible than fit method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpRElLO8Lhwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_model() # create model using methods listed above\n",
        "model.compile(optimizer=optimizers.Nadam(),loss=losses.SparseCategoricalCrossentropy(),metrics=['accuracy']) # compile model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2jTzJT1Ic5-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model,ds_train,ds_valid,epoches):\n",
        "\n",
        "    for epoch in tf.range(1,epoches+1):\n",
        "        model.reset_metrics()\n",
        "        \n",
        "        # Reduce learning rate at the late stage of training.\n",
        "        if epoch == 5:\n",
        "            model.optimizer.lr.assign(model.optimizer.lr/2.0)\n",
        "            tf.print(\"Lowering optimizer Learning Rate...\\n\\n\")\n",
        "        \n",
        "        for x, y in ds_train:\n",
        "            train_result = model.train_on_batch(x, y)\n",
        "\n",
        "        for x, y in ds_valid:\n",
        "            valid_result = model.test_on_batch(x, y,reset_metrics=False)\n",
        "            \n",
        "        if epoch%1 ==0:\n",
        "            printbar()\n",
        "            tf.print(\"epoch = \",epoch)\n",
        "            print(\"train:\",dict(zip(model.metrics_names,train_result)))\n",
        "            print(\"valid:\",dict(zip(model.metrics_names,valid_result)))\n",
        "            print(\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzGU4OqYI9jr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_model(model,ds_train,ds_test,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIkxha4TJANz",
        "colab_type": "text"
      },
      "source": [
        "### 3-3: customized training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCveHEjWJhco",
        "colab_type": "text"
      },
      "source": [
        "Re-compilation of the model is not required in the customized training loop, just back-propagate the iterative parameters through the optimizer according to the loss function, which gives us the highest flexibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSCLSFpMJcDp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = optimizers.Nadam()\n",
        "loss_func = losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "train_loss = metrics.Mean(name='train_loss')\n",
        "train_metric = metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "valid_loss = metrics.Mean(name='valid_loss')\n",
        "valid_metric = metrics.SparseCategoricalAccuracy(name='valid_accuracy')\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, features, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(features,training = True) # prediction\n",
        "        loss = loss_func(labels, predictions) # calculate loss\n",
        "    gradients = tape.gradient(loss, model.trainable_variables) # gradient\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables)) # gradient descent -> optimizer\n",
        "\n",
        "    train_loss.update_state(loss) # update loss\n",
        "    train_metric.update_state(labels, predictions) # update metric\n",
        "    \n",
        "\n",
        "@tf.function\n",
        "def valid_step(model, features, labels):\n",
        "    predictions = model(features)\n",
        "    batch_loss = loss_func(labels, predictions)\n",
        "    valid_loss.update_state(batch_loss)\n",
        "    valid_metric.update_state(labels, predictions)\n",
        "    \n",
        "\n",
        "def train_model(model,ds_train,ds_valid,epochs):\n",
        "    for epoch in tf.range(1,epochs+1):\n",
        "        \n",
        "        for features, labels in ds_train:\n",
        "            train_step(model,features,labels)\n",
        "\n",
        "        for features, labels in ds_valid:\n",
        "            valid_step(model,features,labels)\n",
        "\n",
        "        logs = 'Epoch={},Loss:{},Accuracy:{},Valid Loss:{},Valid Accuracy:{}'\n",
        "        \n",
        "        if epoch%1 ==0:\n",
        "            printbar()\n",
        "            tf.print(tf.strings.format(logs,\n",
        "            (epoch,train_loss.result(),train_metric.result(),valid_loss.result(),valid_metric.result())))\n",
        "            tf.print(\"\")\n",
        "            \n",
        "        train_loss.reset_states()\n",
        "        valid_loss.reset_states()\n",
        "        train_metric.reset_states()\n",
        "        valid_metric.reset_states()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbxyAceDMuFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_model(model,ds_train,ds_test,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_uRmQKmMw1U",
        "colab_type": "text"
      },
      "source": [
        "## 04: Save and use model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynxe3qPGM2Q2",
        "colab_type": "text"
      },
      "source": [
        "4-1. Save the whole model, including the model's architecture, weight values, training config, optimizer and its states.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VGvl8NTTuvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save model as h5 file\n",
        "model.save('mymodel.h5')\n",
        "\n",
        "# load model\n",
        "new_model = tf.keras.models.load_model('mymodel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_ZSfjQuUd1M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save model as SavedModel (recommended)\n",
        "model.save('tf_model_savedmodel', save_format=\"tf\")\n",
        "\n",
        "# load model\n",
        "new_model = tf.keras.models.load_model('tf_model_savedmodel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNonUnLQVFDQ",
        "colab_type": "text"
      },
      "source": [
        "4-2. Save model's architecture and weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGkgy7JbVU5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save model's architecture\n",
        "json_config = model.to_json()\n",
        "with open('model_config.json','w') as json_file:\n",
        "  json_file.write(json_config)\n",
        "# save model's weights\n",
        "#model.save_weights('model_weights.h5')\n",
        "model.save_weights('model_weights',save_format='tf')\n",
        "# load model\n",
        "with open('model_config.json') as json_file:\n",
        "  json_config = json_file.read()\n",
        "new_model = tf.keras.models.model_from_json(json_config)\n",
        "#new_model.load_weights('model_weights.h5')\n",
        "new_model.load_weights('model_weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT06su1eXdJ1",
        "colab_type": "text"
      },
      "source": [
        "Note: If the training process is customized (i.e. not using ```model.fit```), we should set model's input shape before training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MExJab1FX943",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create model\n",
        "model = KeypointNetwork()  \n",
        "\n",
        "# set input shape\n",
        "shape = tf.TensorSpec(shape = (batch_size,210), dtype=tf.dtypes.float32, name=None)\n",
        "model._set_inputs(shape)  \n",
        " \n",
        "# training\n",
        "for epoch in range(1,epochs+1):\n",
        "    ... ...\n",
        "    ... ...\n",
        " \n",
        " \n",
        "# save model\n",
        "model.save('./model-weights/checkpoint_weights',save_format='tf')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}